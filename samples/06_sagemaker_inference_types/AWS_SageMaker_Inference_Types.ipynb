{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1808d3ab-d68d-4079-99b9-4c1efd45b8e3",
   "metadata": {},
   "source": [
    "## AWS SageMaker Inference Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c27e10-351e-421b-bc5b-510c410840b8",
   "metadata": {},
   "source": [
    "This Jupyter Notebook provides a comprehensive exploration of the various AWS SageMaker inference types, offering a detailed overview and practical insights into each type's capabilities, use cases, and considerations. AWS SageMaker is a powerful platform for building, training, and deploying machine learning models, and understanding its inference options is essential for optimizing model deployment.\n",
    "\n",
    "The notebook covers the following AWS SageMaker inference types:\n",
    "\n",
    "**Real-Time Inference**\n",
    "\n",
    "This section delves into the real-time inference capabilities of SageMaker. It explains how to deploy a trained model as an endpoint and make real-time predictions using RESTful APIs. Considerations like endpoint scaling, latency, and cost are discussed, along with best practices for real-time inference scenarios.\n",
    "\n",
    "**Batch Transform***\n",
    "\n",
    "The Batch Transform section explores the use of SageMaker for processing large batches of data for inference. It demonstrates how to set up batch transform jobs, optimize resource allocation, and handle output results efficiently. This type of inference is suitable for scenarios where high throughput and efficiency are paramount.\n",
    "\n",
    "**Multi-Model Endpoints**\n",
    "\n",
    "In this section, the notebook showcases the concept of multi-model endpoints, allowing the deployment of multiple model versions as a single endpoint. It explains how to manage and route requests to specific models based on different criteria, ensuring seamless updates and version control without disrupting user experience.\n",
    "\n",
    "**Multi-Container Endpoints**\n",
    "\n",
    "The Multi-Container Endpoints section illustrates the process of deploying custom inference containers using SageMaker. It demonstrates how to package a custom inference algorithm and deploy it alongside pre-built SageMaker containers, enabling diverse use cases that may require specialized software stacks.\n",
    "\n",
    "**Elastic Inference**\n",
    "\n",
    "Elastic Inference is explored as a means to optimize the cost of deploying deep learning models. This section explains how to attach elastic inference accelerators to SageMaker instances, allowing users to match inference resource requirements with model complexity dynamically.\n",
    "\n",
    "**Asynchronous Inference**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c208503d-e66f-41bb-8eef-f6576cc9bcd4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f77c77b-7328-436c-a7a0-9038178810d3",
   "metadata": {},
   "source": [
    "#### What is SageMaker Inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a4e0d-0373-4651-baa3-f983b90b06a5",
   "metadata": {},
   "source": [
    "Machine learning models play a pivotal role in today's data-driven world, enabling businesses to extract insights, make predictions, and automate decision-making processes. However, the journey from model development to deployment can be complex, requiring a reliable and scalable infrastructure. This is where AWS SageMaker comes into play.\n",
    "\n",
    "Amazon SageMaker is a fully managed machine learning service provided by Amazon Web Services (AWS). It streamlines the process of building, training, and deploying machine learning models, allowing data scientists and developers to focus on the modeling aspects rather than the underlying infrastructure. One of the critical aspects of deploying models with SageMaker is choosing the appropriate inference type based on the specific requirements of the application.\n",
    "\n",
    "**Inference**, in the context of machine learning, refers to the process of using a trained model to make predictions on new, unseen data. AWS SageMaker offers various inference types, each tailored to different scenarios and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3964587-6493-43c1-8bcd-a561917c84ab",
   "metadata": {},
   "source": [
    "### 1. Real-Time Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be92ef9a-c38a-4355-bb5e-86d9475d2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Initialize SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"arn:aws:iam::[ACCOUNT_NUMBER]:role/service-role/AmazonSageMaker-ExecutionRole\"\n",
    "\n",
    "# Specify the location of the trained model artifacts\n",
    "model_artifact = \"s3://your-bucket/model-artifacts/model.tar.gz\"\n",
    "\n",
    "# Create a SageMaker model\n",
    "model = sagemaker.Model(model_data=model_artifact,\n",
    "                        role=role,\n",
    "                        sagemaker_session=sagemaker_session)\n",
    "\n",
    "# Deploy the model as an endpoint\n",
    "predictor = model.deploy(instance_type=\"ml.m4.xlarge\", initial_instance_count=1)\n",
    "\n",
    "# Make predictions using the endpoint\n",
    "input_data = ...\n",
    "result = predictor.predict(input_data)\n",
    "\n",
    "# Clean up: Delete the endpoint\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25b4db0-6f20-4b5a-80f0-320afc4dd79f",
   "metadata": {},
   "source": [
    "Real-time inference, also known as online or synchronous inference, involves making predictions using a machine learning model as soon as new data becomes available. This type of inference is characterized by its low latency and immediate response, making it well-suited for various use cases where timely predictions are crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828e2ff-a99b-4869-a72b-9b12094ac259",
   "metadata": {},
   "source": [
    "#### Real-time Inference in SageMaker Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d130c1f-0814-436d-951e-feba99f0cff2",
   "metadata": {},
   "source": [
    "Amazon SageMaker Pipelines is a feature within Amazon SageMaker that allows you to build, automate, and manage end-to-end machine learning workflows. It enables you to create, deploy, and manage machine learning pipelines as code, making it easier to streamline and automate the various steps involved in the machine learning lifecycle, from data preprocessing to model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5ec028-d555-486b-bd57-40d82c728854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Initialize SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"arn:aws:iam::ACCOUNT_NUMBER:role/service-role/AmazonSageMaker-ExecutionRole\"\n",
    "\n",
    "# Specify the location of the trained model artifacts\n",
    "model_artifact = \"s3://your-bucket/model-artifacts/model.tar.gz\"\n",
    "\n",
    "# Create a SageMaker model\n",
    "model = Model(model_data=model_artifact,\n",
    "              role=role,\n",
    "              sagemaker_session=sagemaker_session)\n",
    "\n",
    "# Deploy the model as an endpoint\n",
    "predictor = model.deploy(instance_type=\"ml.m4.xlarge\", initial_instance_count=1)\n",
    "\n",
    "# Create a SageMaker PipelineModel\n",
    "pipeline_model = PipelineModel(\n",
    "    name=\"example-pipeline-model\",\n",
    "    role=role,\n",
    "    models=[model])\n",
    "\n",
    "# Attach a real-time predictor to the pipeline model\n",
    "class RealTimePredictor(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(RealTimePredictor, self).__init__(\n",
    "            endpoint_name,\n",
    "            sagemaker_session=sagemaker_session,\n",
    "            serializer=JSONSerializer(),\n",
    "            deserializer=JSONDeserializer())\n",
    "\n",
    "# Use the pipeline model with the real-time predictor\n",
    "real_time_predictor = RealTimePredictor(endpoint_name=pipeline_model.endpoint_name,\n",
    "                                        sagemaker_session=sagemaker_session)\n",
    "\n",
    "# Make predictions using the real-time predictor\n",
    "input_data = ...\n",
    "result = real_time_predictor.predict(input_data)\n",
    "\n",
    "# Clean up: Delete the endpoint\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735175c7-02de-4aaf-bbd7-2acee46b30b9",
   "metadata": {},
   "source": [
    "#### When to use real-time inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9799f54-ea1c-466c-882a-d54b60086f3b",
   "metadata": {},
   "source": [
    "Use real-time inference when your application requires immediate responses to user interactions, such as recommendation systems, chatbots, fraud detection, and real-time analytics, real-time inference ensures a seamless and responsive user experience. If your data is dynamic and changes frequently, real-time inference allows you to make predictions using the most up-to-date information, ensuring accurate and relevant results.\n",
    "\n",
    "Applications that involve real-time decision-making, such as autonomous vehicles, industrial automation, and IoT devices, benefit from real-time inference to enable instant responses and adapt to changing conditions. Use cases with stringent latency requirements, such as high-frequency trading, medical diagnosis, and online gaming, demand real-time inference to meet the sub-second response times.\n",
    "\n",
    "Real-time inference is essential for quickly detecting anomalies or deviations from expected patterns, allowing immediate action to mitigate potential issues. Applications that provide immediate feedback to users, like language translation, speech recognition, and image recognition, rely on real-time inference to enhance user interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba467876-7858-4561-b85f-4876f582e710",
   "metadata": {},
   "source": [
    "#### When NOT to use real-time inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72335eeb-b3e1-423b-904f-12fbcf6861b4",
   "metadata": {},
   "source": [
    " If you have a large batch of data to process and latency is not a critical factor, batch inference might be more efficient. Batch inference can process data in parallel and is optimized for high throughput. If you have predictable or scheduled inference workloads, you can plan ahead and use batch or scheduled inference jobs to optimize resource utilization. Real-time inference might not be suitable for use cases with extremely high inference rates where the volume of requests overwhelms the real-time endpoint, causing latency spikes and reduced performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1c00d-9d72-4edf-8e2b-87c1ac567c5e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b931726d-8358-4287-910e-3bf22ca83e0d",
   "metadata": {},
   "source": [
    "### 2. Batch Transform Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ad1f8-b341-47c9-992f-b2279d061fcf",
   "metadata": {},
   "source": [
    "It allows you to perform large-scale, batch processing of data using trained machine learning models. Unlike real-time inference, where predictions are made immediately upon receiving new data, batch transform is designed for scenarios where you have a large amount of data that needs to be processed offline in bulk.\n",
    "\n",
    "In Batch Transform, you provide the input data in batches, and SageMaker processes these batches using the trained model to generate predictions or other desired outputs. Batch Transform is particularly useful when low latency is not a requirement, and you can afford to wait for the processing to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc8e3af-51ae-4877-9bb5-8639bbbdbd6b",
   "metadata": {},
   "source": [
    "Input data for Batch Transform must be stored in Amazon S3 buckets. SageMaker supports various input formats, such as CSV, JSON, and Parquet. The data should be organized in a way that's compatible with the model's input requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e272f7-cd27-49c4-8a30-5f1ed935030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "# Initialize SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"arn:aws:iam::ACCOUNT_NUMBER:role/service-role/AmazonSageMaker-ExecutionRole\"\n",
    "\n",
    "# Specify the location of the trained model artifacts\n",
    "model_artifact = \"s3://your-bucket/model-artifacts/model.tar.gz\"\n",
    "\n",
    "# Specify the input data location\n",
    "input_data = \"s3://your-bucket/input-data/input.csv\"\n",
    "\n",
    "# Specify the output data location\n",
    "output_data = \"s3://your-bucket/output-data/\"\n",
    "\n",
    "# Create a transformer object\n",
    "transformer = Transformer(model_name=\"your-model-name\",\n",
    "                          instance_count=1,\n",
    "                          instance_type=\"ml.m4.xlarge\",\n",
    "                          strategy=\"SingleRecord\",\n",
    "                          assemble_with=\"Line\",\n",
    "                          output_path=output_data,\n",
    "                          base_transform_job_name=\"batch-transform-job\")\n",
    "\n",
    "# Start the batch transform job\n",
    "transformer.transform(data=input_data,\n",
    "                      data_type=\"S3Prefix\",\n",
    "                      content_type=\"text/csv\",\n",
    "                      split_type=\"Line\")\n",
    "\n",
    "# Wait for the job to complete\n",
    "transformer.wait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584b3d09-87af-422d-82d4-07488e9aeeb0",
   "metadata": {},
   "source": [
    "Parameters of the transform() method:\n",
    "\n",
    "**model_name**\n",
    "This parameter specifies the name of the trained SageMaker model that you want to use for batch transformation.\n",
    "\n",
    "**instance_count**\n",
    "The number of instances to use for batch transformation. This determines the level of parallelism and resource allocation for the transformation job.\n",
    "\n",
    "**instance_type** \n",
    "The type of instance to use for batch transformation. This determines the computing power and memory available for the transformation job.\n",
    "\n",
    "**strategy** \n",
    "The transformation strategy to use. In this example, \"SingleRecord\" indicates that each input record is transformed independently. Other options include \"MultiRecord\" for processing multiple records at once and \"Serial\" for serial processing.\n",
    "\n",
    "**assemble_with** \n",
    "Specifies how the output is assembled. \"Line\" assembles the output as a single line with predictions separated by newlines. Other options include \"None\" and \"LineOfCSV\".\n",
    "\n",
    "**output_path** \n",
    "The S3 location where the transformed data will be stored after processing. This is where the results of the batch transformation will be saved.\n",
    "\n",
    "**data**\n",
    "The S3 location of the input data.\n",
    "\n",
    "**data_type**\n",
    "The data type of the input data. Here, \"S3Prefix\" indicates that the input data is located in an S3 prefix.\n",
    "\n",
    "**content_type**\n",
    "The content type of the input data, such as \"text/csv\" in this example.\n",
    "\n",
    "**split_type**\n",
    "The type of splitting to apply to the input data. Here, \"Line\" indicates that each line of the input data is treated as a separate record.\n",
    "wait method: Waits for the batch transform job to complete. It ensures that the code execution doesn't proceed until the job has finished processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b437b5-6ed4-48ff-9a07-46aecf119d7c",
   "metadata": {},
   "source": [
    "### Batch Transform Limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5ce66-8e6b-40a7-aa4b-929c30a0a1a6",
   "metadata": {},
   "source": [
    "Amazon SageMaker Batch Transform imposes a limit on the input payload size, which cannot exceed 100 MB. If you encounter an error related to the input size being larger than 100 MB, you'll need to consider strategies to work within this limitation. Here are a few approaches you can take: \n",
    "\n",
    "If your input data is too large to fit within the 100 MB limit, consider preprocessing the data to reduce its size. You might also consider sampling a representative subset of the data for batch transformation. Another solution could be: break your larger input data into smaller chunks or batches that each fall within the 100 MB limit. You can then process these chunks sequentially using multiple batch transform jobs. Streaming data, on the other hand, can be a workaround. Instead of processing the entire dataset in one go, consider streaming the data in smaller segments. You can use Amazon S3's SELECT feature to filter and retrieve only the necessary data for each batch transform job. And finally, the most common way would be splitting input data: If your data can be split into distinct subsets, you can run separate batch transform jobs for each subset. This approach can help you avoid the 100 MB limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387b99eb-7c17-4597-ba59-4340ba1f2b57",
   "metadata": {},
   "source": [
    "An example of batch transform step in SageMaker pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d957f917-d8e3-4677-9824-ffa4bf65bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import Pipeline, PipelineModel\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# Initialize SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"arn:aws:iam::ACCOUNT_NUMBER:role/service-role/AmazonSageMaker-ExecutionRole\"\n",
    "\n",
    "# Specify the location of the trained model artifacts\n",
    "model_artifact = \"s3://your-bucket/model-artifacts/model.tar.gz\"\n",
    "\n",
    "# Specify the input data location\n",
    "input_data = \"s3://your-bucket/input-data/input.csv\"\n",
    "\n",
    "# Specify the output data location\n",
    "output_data = \"s3://your-bucket/output-data/\"\n",
    "\n",
    "# Create a Transformer object\n",
    "transformer = Transformer(model_name=\"your-model-name\",\n",
    "                          instance_count=1,\n",
    "                          instance_type=\"ml.m4.xlarge\",\n",
    "                          strategy=\"SingleRecord\",\n",
    "                          assemble_with=\"Line\",\n",
    "                          output_path=output_data,\n",
    "                          base_transform_job_name=\"batch-transform-job\")\n",
    "\n",
    "# Define a Batch Transform step\n",
    "batch_transform_step = sagemaker.processing.TrainingStep(\n",
    "    name=\"BatchTransformStep\",\n",
    "    transformer=transformer,\n",
    "    inputs=[sagemaker.processing.ProcessingInput(\n",
    "        source=input_data,\n",
    "        destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[sagemaker.processing.ProcessingOutput(output_name=\"output\", source=\"/opt/ml/processing/output\")]\n",
    ")\n",
    "\n",
    "# Create a SageMaker pipeline\n",
    "pipeline = Pipeline(\n",
    "    name=\"BatchTransformPipeline\",\n",
    "    steps=[batch_transform_step]\n",
    ")\n",
    "\n",
    "# Execute the pipeline\n",
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2e8fe6-6e09-4a31-a2e0-81e783135da7",
   "metadata": {},
   "source": [
    "Model artifact refers to the result of training a machine learning model. It includes the trained model parameters, weights, and other artifacts that define the model's learned behavior. The model artifact is a key component required for deploying and using a trained model for making predictions or performing other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dec8bf-69e1-457a-b7d8-9d501adae421",
   "metadata": {},
   "source": [
    "#### Three ways to tune batch transform job:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469eddac-72c7-49b7-9fdd-28b071d5d8dc",
   "metadata": {},
   "source": [
    "**Strategy** \n",
    "Choose the right transformation strategy based on your input data and use case. Strategies like \"MultiRecord\" can be more efficient for larger datasets, while \"SingleRecord\" is suitable for individual records or small batches.\n",
    "\n",
    "**Data Format**\n",
    "Optimize your input and output data formats. Choose data formats that are efficient for processing and storage. For example, Parquet or binary formats might be more efficient than plain text for large datasets.\n",
    "\n",
    "**Output Format**\n",
    "Consider how you want the output data to be assembled and organized. The output format you choose (\"Line\", \"LineOfCSV\", \"None\") can affect the final result and post-processing efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b9ccf-1c4a-475f-a2da-31ab49735caf",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb13fc4e-738a-43f9-986c-54b5f345ef8f",
   "metadata": {},
   "source": [
    "### 3. Multi-Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cfcb8e-6884-48c4-9529-7cc532f7f23c",
   "metadata": {},
   "source": [
    "A multi-model endpoint in Amazon SageMaker is an endpoint that allows you to deploy and serve multiple machine learning models behind a single endpoint. This feature is particularly useful when you have a variety of models that you want to deploy together for inference. Instead of creating separate endpoints for each model, you can consolidate them into a single multi-model endpoint, which can save resources, simplify deployment, and improve management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9570cb-00e8-40a7-b006-7f60dc656e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.multi_model import MultiModel\n",
    "\n",
    "# Initialize SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"arn:aws:iam::ACCOUNT_NUMBER:role/service-role/AmazonSageMaker-ExecutionRole\"\n",
    "\n",
    "# Specify the location of the trained model artifacts\n",
    "model_artifact_1 = \"s3://your-bucket/model1-artifacts/model.tar.gz\"\n",
    "model_artifact_2 = \"s3://your-bucket/model2-artifacts/model.tar.gz\"\n",
    "\n",
    "# Create SageMaker models for each registered model\n",
    "model_1 = Model(model_data=model_artifact_1,\n",
    "                role=role,\n",
    "                sagemaker_session=sagemaker_session)\n",
    "model_2 = Model(model_data=model_artifact_2,\n",
    "                role=role,\n",
    "                sagemaker_session=sagemaker_session)\n",
    "\n",
    "# Create a MultiModel object and register the models\n",
    "multi_model = MultiModel(name=\"your-multi-model-endpoint\",\n",
    "                         model=model_1,\n",
    "                         sagemaker_session=sagemaker_session)\n",
    "multi_model.add_model(model_2)\n",
    "\n",
    "# Deploy the multi-model endpoint\n",
    "multi_model.deploy(initial_instance_count=1,\n",
    "                   instance_type=\"ml.m4.xlarge\")\n",
    "\n",
    "# Make inference requests to the multi-model endpoint\n",
    "input_data = ...\n",
    "result = multi_model.predict(input_data)\n",
    "\n",
    "# Clean up: Delete the multi-model endpoint\n",
    "multi_model.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d6b73c-3b34-4148-a29d-79660f2f78f6",
   "metadata": {},
   "source": [
    "#### When to use Multi-Model?\n",
    "\n",
    "\n",
    "If you want to deploy an ensemble of models, where multiple models work together to make predictions and improve overall accuracy, a multi-model endpoint allows you to serve them as a unified endpoint. When you have multiple versions of the same model, a multi-model endpoint can help you manage and serve different versions without the need for separate endpoints. If you have a variety of models with different computational requirements, serving them together in a single endpoint can be more resource-efficient compared to deploying separate endpoints for each model. For experimentation and A/B testing purposes, you can deploy multiple models simultaneously and route a portion of the inference traffic to each model, allowing you to compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81300ff-9c15-4235-9f50-22cb6a5b8b68",
   "metadata": {},
   "source": [
    "##### Note to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e035ffd-e3fa-4c41-b403-62d7d9dad9bb",
   "metadata": {},
   "source": [
    "The inference payload must include a field that specifies the model name to be used for prediction. This requires modifying the client application to include this information in the request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0076ac00-6eeb-4350-b61b-1a5546a53cbb",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a4768f-487b-46bf-9fa2-13c24dbe3675",
   "metadata": {},
   "source": [
    "### 4. Elastic Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f9c49-a784-4021-acd3-e3401d4dce65",
   "metadata": {},
   "source": [
    "Amazon SageMaker Elastic Inference is a feature that allows you to attach just the right amount of GPU-powered inference acceleration to your Amazon SageMaker instances. It helps optimize the cost and performance trade-off by dynamically adjusting the GPU resources allocated to your inference workloads. Elastic Inference enables you to accelerate model inference while paying only for the GPU resources you use, making it more cost-effective than using a dedicated GPU instance for every inference workload.\n",
    "\n",
    "Elastic Inference works by attaching GPU accelerators to SageMaker instances on-the-fly, without requiring you to explicitly provision dedicated GPU instances. You can choose the amount of GPU resources you need for your specific inference workload, and SageMaker will dynamically allocate the appropriate number of Elastic Inference accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419bf5d9-4670-4914-b443-d7c3a35c29a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "\n",
    "# Initialize SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"arn:aws:iam::ACCOUNT_NUMBER:role/service-role/AmazonSageMaker-ExecutionRole\"\n",
    "\n",
    "# Specify the location of the trained model artifacts\n",
    "model_artifact = \"s3://your-bucket/model-artifacts/model.tar.gz\"\n",
    "\n",
    "# Create a TensorFlowModel\n",
    "tf_model = TensorFlowModel(model_data=model_artifact,\n",
    "                           role=role,\n",
    "                           framework_version=\"2.4.1\",\n",
    "                           sagemaker_session=sagemaker_session)\n",
    "\n",
    "# Define Elastic Inference settings\n",
    "ei_config = {\n",
    "    'AcceleratorType': 'ml.eia1.medium'  # Specify the Elastic Inference accelerator type\n",
    "}\n",
    "\n",
    "# Deploy the model with Elastic Inference\n",
    "predictor = tf_model.deploy(initial_instance_count=1,\n",
    "                            instance_type=\"ml.m5.large\",  # Specify the instance type\n",
    "                            accelerator_type=\"ml.eia1.medium\",  # Attach Elastic Inference accelerator\n",
    "                            accelerator_count=1,  # Number of accelerators\n",
    "                            endpoint_name=\"your-endpoint-name\",\n",
    "                            endpoint_config_name=\"your-endpoint-config-name\",\n",
    "                            accelerator_config=ei_config)\n",
    "\n",
    "# Make inference requests\n",
    "input_data = ...\n",
    "result = predictor.predict(input_data)\n",
    "\n",
    "# Delete the endpoint\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe717486-18bd-4ad4-831f-c1ec93e744b5",
   "metadata": {},
   "source": [
    "To gain maximum performance, you might want to consider choosing elastic inference instance types:\n",
    "\n",
    "Elastic Inference Accelerated Instances:\n",
    "\n",
    "> ml.m5.large, ml.m5.xlarge, ml.m5.2xlarge, ml.m5.4xlarge, ml.m5.12xlarge, ml.m5.24xlarge\n",
    "\n",
    "These instances can be augmented with Elastic Inference accelerators to balance cost and performance for inference workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee90bc-19f9-4328-b4a3-1be160034d6a",
   "metadata": {},
   "source": [
    "The choice of accelerator type depends on factors such as the complexity of your model, the desired latency, and your budget constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99efef49-e162-403c-9724-4b760e7538f7",
   "metadata": {},
   "source": [
    "#### When should we use Elastic Inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0ad6a-8826-43f4-9686-ecf724248cfc",
   "metadata": {},
   "source": [
    "Consider using Amazon SageMaker Elastic Inference when you want to optimize the performance and cost of your inference workloads by attaching GPU acceleration to your SageMaker instances. Elastic Inference is particularly useful in scenarios where dedicated GPU instances might be overprovisioned for your inference requirements, leading to higher costs, or where you want to find the right balance between cost and performance. \n",
    "\n",
    "If your inference workloads vary in complexity and demand, Elastic Inference allows you to dynamically allocate GPU resources as needed without provisioning dedicated GPU instances. Moreover, Elastic Inference helps you optimize costs by providing GPU acceleration only when required. It can be more cost-effective than using full GPU instances for all workloads.\n",
    "\n",
    "When you need to scale your inference resources up or down quickly based on demand, Elastic Inference provides flexibility without the need to launch or terminate instances. For low-latency applications, Elastic Inference can help accelerate your inference workloads without the overhead of provisioning full GPU instances. If your models are relatively lightweight and don't require the full power of dedicated GPU instances, Elastic Inference can be a cost-efficient choice. When deploying multiple models in a single endpoint, Elastic Inference can help balance resource allocation among the models without over-provisioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b75e4f8-60d9-4afe-a73a-136d04799cfd",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e00f52-32eb-4d37-bf03-6eb4cafdf440",
   "metadata": {},
   "source": [
    "### 5. Asynchronous Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceebf4c-3d5c-4619-a689-a0d057801069",
   "metadata": {},
   "source": [
    "This type of inference allows you to perform inference on large batches of data in an asynchronous and parallelized manner. It is particularly useful when you have a substantial amount of data to process and want to optimize the inference process by submitting multiple inference requests simultaneously.\n",
    "\n",
    "With asynchronous inference, you submit a batch of inference requests to SageMaker, and the service processes these requests in the background, independently of the client application. This helps improve efficiency and throughput, as you can take advantage of parallel processing and offload the client from waiting for each individual inference to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743dae9a-72d0-4a51-89d5-82c29eec4b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlowPredictor\n",
    "\n",
    "# Initialize SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"arn:aws:iam::ACCOUNT_NUMBER:role/service-role/AmazonSageMaker-ExecutionRole\"\n",
    "\n",
    "# Create a TensorFlowPredictor\n",
    "predictor = TensorFlowPredictor(endpoint_name=\"your-endpoint-name\",\n",
    "                               sagemaker_session=sagemaker_session,\n",
    "                               role=role)\n",
    "\n",
    "# Prepare a batch of input data for asynchronous inference\n",
    "input_data_batch = [...] # List of dict\n",
    "\n",
    "# Perform asynchronous inference\n",
    "results = predictor.predict(input_data_batch, initial_args={\"Accept\": \"application/jsonlines\"})\n",
    "\n",
    "# Wait for inference to complete (optional)\n",
    "results.wait()\n",
    "\n",
    "# Process inference results\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "# Delete the endpoint\n",
    "predictor.delete_endpoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03acdb-8006-4e80-8ec2-613266a97acd",
   "metadata": {},
   "source": [
    "#### What are the limitations of asynchronous inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0fc1c3-56d2-4239-9f4b-b836ec1382f8",
   "metadata": {},
   "source": [
    "Asynchronous inference processes inference requests in parallel, so the order of results in the response might not match the order of input requests. You need to handle result ordering appropriately when processing the output. The response format depends on the content type specified in the request. You need to ensure that your application can correctly parse and interpret the response format. What is more, performing asynchronous inference with large batches of data requires sufficient compute resources. You might need to optimize the instance type and the number of instances based on your workload.\n",
    "\n",
    "On the other hand, scaling the endpoint to handle a large number of concurrent asynchronous requests might require careful configuration and monitoring to ensure resource availability and efficient processing. While asynchronous inference can improve throughput, individual inferences within the batch might still experience varying latencies, depending on the workload and resource availability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd1316-bd39-4f88-9939-88b328b9be19",
   "metadata": {},
   "source": [
    "### Tips when using asynchronous inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a45407b-4d42-4fd2-ba24-bee86ff2801b",
   "metadata": {},
   "source": [
    "**Retries**\n",
    "\n",
    "When performing asynchronous inference, multiple inference requests are processed in parallel. This parallel processing introduces the possibility of failures, such as network issues, instance failures, or model errors, for individual inferences within the batch. To handle these failures, you might need to implement a retry mechanism. Retries involve resubmitting the failed inference requests to the endpoint in the hope that they will eventually succeed.\n",
    "\n",
    "Considerations for implementing retries:\n",
    "\n",
    "Determine an appropriate number of retry attempts. Too many retries might cause excessive delay, while too few retries might result in missed inferences. Implement an exponential backoff strategy, where you gradually increase the time between retries to avoid overloading the endpoint. Set a maximum retry timeout to prevent waiting indefinitely for a response.\n",
    "\n",
    "**Timeouts**\n",
    "\n",
    "Timeouts are especially important in asynchronous inference scenarios because processing a large batch of data might take longer than processing individual synchronous inferences. Specifying an appropriate timeout ensures that your client application doesn't wait excessively for a response.\n",
    "\n",
    "Considerations for setting timeouts:\n",
    "\n",
    "Set the timeout value based on your model's processing time and the expected range of batch sizes. Larger batches might require longer timeouts. Ensure that your timeout value is reasonable to avoid prematurely giving up on inferences that are still being processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edcf7ef-0c51-4fa4-93b5-daa650811cea",
   "metadata": {},
   "source": [
    "**An example scenario:**\n",
    "\n",
    "Suppose you're using a client application to submit a batch of 1000 inference requests to an asynchronous inference endpoint. The model's processing time per inference is around 2 seconds on average. You set a timeout of 15 minutes (900 seconds) for the batch. If all inferences complete successfully, the response should be received well before the timeout. If some inferences fail initially, your retry mechanism could resubmit the failed ones. However, keep in mind that retries introduce additional time. If your retry mechanism is successful, you should still receive a response within the timeout. \n",
    "\n",
    "In this example, setting an appropriate timeout and implementing retries are crucial to ensure that your client application efficiently handles asynchronous inference, including potential failures and variations in processing times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72545bac-8ded-4082-a06f-44df21b0f013",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045edd4c-abfc-407e-95d0-99fab4e24b44",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc329ea6-cb79-4ec9-91a5-4da4a040808d",
   "metadata": {},
   "source": [
    "SageMaker offers a streamlined process for model deployment, eliminating the complexities of managing infrastructure and scaling resources. This enables data scientists and developers to focus on their core tasks rather than the operational overhead. Additionally, SageMaker supports various inference options, including real-time, batch, and asynchronous modes, accommodating a wide range of use cases. Its ability to seamlessly integrate with other AWS services ensures data security, scalability, and robustness. Moreover, SageMaker provides a rich set of monitoring and debugging tools, allowing users to track performance, detect anomalies, and optimize resource utilization. \n",
    "\n",
    "Overall, leveraging SageMaker for inference empowers organizations to efficiently and cost-effectively transform trained models into valuable insights and predictions, accelerating their journey from data science to production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81c7209-6a44-4107-af51-d6ab62e04024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
